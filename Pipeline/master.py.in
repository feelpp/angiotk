#!/usr/bin/env python

import argparse
import ConfigParser
import datetime
import errno
import glob
import os
import shutil
import distutils.dir_util
import stat
import multiprocessing as mp
import subprocess
import sys
import threading
import time
# to generate results database (json file)
import json
from collections import OrderedDict
import string

import testAngioTKPipeline

def main():

    # define scripts paths (partially filled by cmake during install)
    testAngioTK = os.path.expandvars('@CMAKE_INSTALL_PREFIX@/bin/testAngioTKPipeline.py')
    asciidocDBpy = os.path.expandvars('@CMAKE_INSTALL_PREFIX@/bin/createAsciidocDB.py')
    summaryHtml = os.path.expandvars('@CMAKE_INSTALL_PREFIX@/bin/summary.html')
    angioTKVersion = string.rstrip('@CMAKE_ANGIOTK_VERSION@', '\n')

    # write down current time (useful to time the process)
    initialTime = datetime.datetime.now()

    # arguments parsing and management
    parser = argparse.ArgumentParser()
    parser.add_argument('--inputpath', required=True, help='Path to input files (mha or nii format)')
    parser.add_argument('-f', default="mha", help='Format of input files to process: nii or mha (default: mha)')
    parser.add_argument('--outputpath', required=True, help='Path to write results to')
    parser.add_argument('-n', default=1, type=int, help='Limit actual processing to n mha files')
    parser.add_argument('-p', default=1, type=int, help='Number of processes to use')
    parser.add_argument('--noscreenshots', dest='enablescreenshots', action='store_false', help='Disable screenshots.')
    parser.set_defaults(enablescreenshots=True)
    args = parser.parse_args() # parse arguments into objects and store them as attributes of a namespace

    databaseRootDir = args.inputpath
    resultsPath = args.outputpath
    mustSaveScreenshots = args.enablescreenshots
    print 'Will save screenshots: ' + str(mustSaveScreenshots)

    # find MRI files by exploring database root directory
    mhaFiles = []
    osWalkInitTime = time.time()
    for path, dirs, files in os.walk(databaseRootDir):
        for f in files:
            if f.endswith("art." + args.f) or f.endswith("cor." + args.f) or f.endswith("pcp." + args.f):
                mhaFiles.append(os.path.join(path,f))
    osWalkEndTime = time.time()
    print "The following .mha files were found"
    for f in mhaFiles:
        print f

    print "os.walk total time: " + str(round(osWalkEndTime-osWalkInitTime,1))

    # results database initialisation: resultsPath is the root directory of all results ($RESULTS)
    resultsDataBasePath = os.path.join(resultsPath, 'resultsDataBase', '') # ex: $RESULTS/resultsDataBase/
    testAngioTKPipeline.makedir(resultsDataBasePath)
    # create a subdirectory for logs
    resultsLogPath = os.path.join(resultsPath, "logs", '')
    testAngioTKPipeline.makedir(resultsLogPath)
    summary_json = os.path.join(resultsDataBasePath, 'summary.json') # ex: $RESULTS/resultsDataBase/summary.json
    # we group all runs in a directory
    runsPath = os.path.join(resultsDataBasePath, 'runs') # ex: $RESULTS/resultsDataBase/runs
    testAngioTKPipeline.makedir(runsPath)
    # for a given run, we use its launch date and time as ID and directory name (YYYY-MM-DD_HH:MM:SS)
    timeID = (datetime.datetime.today()).strftime("%Y-%m-%d_%H-%M-%S")
    runID = "run-" + timeID
    resultsDBPath = os.path.join(runsPath, runID) # ex: $RESULTS/resultsDataBase/runs/run-YYYY-MM-DD_HH:MM:SS
    testAngioTKPipeline.makedir(resultsDBPath)
    partialResultsDBPath = os.path.join(resultsDBPath, 'partial_results')
    testAngioTKPipeline.makedir(partialResultsDBPath)
    resultsDBName = "resultsDB.json"
    results_json = os.path.join(resultsDBPath, resultsDBName) # ex: $RESULTS/resultsDataBase/runs/run-YYYY-MM-DD_HH:MM:SS/resultsDB.json
    results_html_absolute = os.path.splitext(results_json)[0] + '.html' # ex: $RESULTS/resultsDataBase/runs/run-YYYY-MM-DD_HH:MM:SS/resultsDB.html
    results_html_extra_slash = results_html_absolute.lstrip(resultsDataBasePath) # ex: /runs/run-YYYY-MM-DD_HH:MM:SS/resultsDB.html
    results_html = os.path.join('runs', runID, os.path.splitext(resultsDBName)[0] + '.html') # ex: runs/run-YYYY-MM-DD_HH:MM:SS/resultsDB.html

    partialResultsToMerge = []

    i = 0
    limit = min(args.n, len(mhaFiles))
    # if limit == 0, no MRI file is processed, but results database is created.
    if limit > -1:
        # initialize database by reading existing file or creating one if needed.
        resultsDB = OrderedDict() # Data structure: ordered dictionary
        try: # if .json already exists, read it
            dbFile = open(results_json,'r')
            resultsDB = json.load(dbFile, object_pairs_hook=OrderedDict)
            dbFile.close()
        except IOError, e:
            print("The file " + results_json + " was not found, it will be created.")
        except Exception, e:
            print(e)
        # adds the configuration entry if needed
        cfg = 'configuration'
        resultsDB[cfg]=OrderedDict()
        # fills the configuration entry
        resultsDB[cfg]['date'] = timeID
        resultsDB[cfg]['database'] = args.inputpath
        out = subprocess.check_output(["uname", "-a"])
        resultsDB[cfg]['system'] = string.rstrip(out, '\n') # uname's output ends with '\n', we strip this away
        out = subprocess.check_output("whoami")
        resultsDB[cfg]['user'] = string.rstrip(out, '\n') # whoami's output also ends with '\n', we strip this away
        resultsDB[cfg]['angiotk version'] = angioTKVersion # AngioTK version (git commit)
        p = subprocess.Popen(["python", "-V"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = p.communicate()
        resultsDB[cfg]['python'] = string.rstrip(err, '\n') # python version is printed in stderr, not in stdout !
        p = subprocess.Popen(["pvpython", "-V"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, err = p.communicate()
        resultsDB[cfg]['paraview'] = string.rstrip(err, '\n')
        resultsDB[cfg]['found files'] = str(len(mhaFiles))
        resultsDB[cfg]['processed files'] = 0
        resultsDB[cfg]['successful reconstructions'] = 0
        resultsDB[cfg]['incomplete reconstructions'] = 0
        resultsDB[cfg]['total failure'] = 0
        resultsDB[cfg]['total time'] = str(datetime.datetime.now()-initialTime)
        resultsDB[cfg]['processes'] = args.p
        resultsDB[cfg]['link'] = results_html
        #resultsDB[cfg]['command lines'] = ""
        #commandLines = OrderedDict()
        print cfg + ": "
        print json.dumps(resultsDB[cfg], indent=4)

        dbFile = open(results_json, 'w')
        json.dump(resultsDB, dbFile, indent=4)
        dbFile.close()

        processingPool = mp.Pool(processes=args.p)

        # loop for all MRI files to process
        for mhaFile in mhaFiles: # ex: /data/vivabrain/IRM/Base2/TOF/natives/MHA/GEO26/TOF_art.mha
            if(i==limit): # This is the limit set in the command line's arguments
                break
            # pathNoExt, ext = os.path.splitext(mhaFile) # ex: separate /data/vivabrain/IRM/Base2/TOF/natives/MHA/GEO26/TOF_art and .mha
            pathToMha = string.rstrip(string.lstrip(mhaFile, '/data/vivabrain/IRM/')) # only keep Base2/TOF/natives/MHA/GEO26/TOF_art.mha
            fileIDNoSlash = pathToMha.replace('/','_').replace('.','_') # turn it into Base2_TOF_natives_MHA_GEO26_TOF_art_mha
            # assemble the configuration files path
            # (We don't create this directory. If it does not exists, testAngioTKPipeline.py will use the generic cfg path.)
            inputPath = os.path.join(os.path.expandvars('@CMAKE_INSTALL_PREFIX@/share/AngioTK/Examples/MeshFromMRI'), fileIDNoSlash)
            # path to this file's results database (pipeline metadata for this MRI file)
            fileResultsDBPath = os.path.join(partialResultsDBPath, (fileIDNoSlash + '.json'))
            partialResultsToMerge.append(fileResultsDBPath)

            # pipeline command assembly
            cmd = "python " + testAngioTK + ' --inputfile=' + mhaFile + ' --inputpath='+ inputPath  + ' --outputpath=' + os.path.join(resultsPath, fileIDNoSlash) + ' --resultspath=' + resultsDBPath + ' --dbpath=' + fileResultsDBPath
            if not mustSaveScreenshots:
                cmd = cmd + ' --noscreenshots'
            print 'Processing file ' + str(i+1) + '/' + str(limit) + ': ' + mhaFile + '...'
            print 'cmd: ' + cmd
            print '----------------------------------------' + testAngioTK + '--------------------------------------------'
            sys.stdout.flush()

            # resultsDB update
            try: # read json file
                dbFile = open(results_json,'r')
                resultsDB = json.load(dbFile, object_pairs_hook=OrderedDict)
                dbFile.close()
            except Exception, e:
                print(e)

            # increase the number of processed files:
            # this is done before launching the pipeline
            # to correctly count aborted runs
            resultsDB[cfg]['processed files'] = i+1 # i=0 initially

            # write to database file
            dbFile = open(results_json, 'w')
            json.dump(resultsDB, dbFile, indent=4)
            dbFile.close()

            # path to this file's pipeline logs
            fileLogsDir = os.path.join(resultsLogPath, fileIDNoSlash)
            testAngioTKPipeline.makedir(fileLogsDir)
            fileLog = os.path.join(fileLogsDir, timeID)
            # Actual pipeline execution
            processingPool.apply_async( testAngioTKPipeline.executeCommand, ("", cmd, fileLog) )
            #testAngioTKPipeline.executeCommand("", cmd, resultsLogPath)


            # resultsDB update
            try: # read json file
                dbFile = open(results_json,'r')
                resultsDB = json.load(dbFile, object_pairs_hook=OrderedDict)
                dbFile.close()
            except Exception, e:
                print(e)
                sys.stdout.flush()

            # Update results database
            #commandLines[fileIDNoSlash] = cmd
            #resultsDB[cfg]['command lines'] = json.dumps(commandLines, indent=4)
            resultsDB[cfg]['total time'] = str(datetime.datetime.now()-initialTime)

            # write to database file
            dbFile = open(results_json, 'w')
            json.dump(resultsDB, dbFile, indent=4)
            dbFile.close()

            i=i+1


    # pool closing
    sys.stdout.flush()
    print("Closing processing pool...")
    processingPool.close()
    print("waitin for processes to finish...")
    processingPool.join()
    print("all processes are done. Merging results...")
    sys.stdout.flush()

    # results merging
    print "partial jsons:"
    resultsDB['results'] = OrderedDict()
    for partial_json in partialResultsToMerge:
        print partial_json
        try: # read json file
            dbFile = open(partial_json,'r')
            partialDB = json.load(dbFile, object_pairs_hook=OrderedDict)
            dbFile.close()
        except Exception, e:
            print(e)

        if not partialDB.has_key('results'):
            break
        for fileKey, fileValues in partialDB['results'].items():
            resultsDB['results'][fileKey] = fileValues

    # Database final update
    nSuccess = 0
    nIncomplete = 0
    nFailure= 0
    for mriFile in resultsDB['results'].keys(): # For each mriFile
        if resultsDB['results'][mriFile]['1 - RORPO processing']['Success']==False:
            nFailure += 1 # total failures
        else:
            if resultsDB['results'][mriFile]['6 - Volume mesh processing']['Success']==True:
                nSuccess += 1 # completed reconstructions
            else:
                nIncomplete += 1 # incomplete reconstructions
    resultsDB[cfg]['successful reconstructions'] = nSuccess
    resultsDB[cfg]['incomplete reconstructions'] = nIncomplete
    resultsDB[cfg]['total failure'] = nFailure
    resultsDB[cfg]['total time'] = str(datetime.datetime.now()-initialTime)
    # write to database file
    dbFile = open(results_json, 'w')
    json.dump(resultsDB, dbFile, indent=4)
    dbFile.close()

    # Summary database reading (including previous runs)
    summaryDB = OrderedDict() # Data structure: ordered dictionary
    oldRuns = OrderedDict() # To store old runs and reverse order
    try: # if summary.json already exists, read it
        summaryFile = open(summary_json,'r')
        summaryDB = json.load(summaryFile, object_pairs_hook=OrderedDict)
        summaryFile.close()
        # store previous runs
        oldRuns = OrderedDict(summaryDB['runs'])
    except IOError, e:
        print("The file " + summary_json + " was not found, it will be created.")
    except Exception, e:
        print(e)

    # create/update runs history:
    summaryDB['runs']=OrderedDict() # start with a new runs database
    summaryDB['runs'][runID] = resultsDB[cfg] # add current run
    while len(oldRuns.keys()) > 0: # copy previous runs back
        k,v = oldRuns.popitem(last=False)
        summaryDB['runs'][k]=v
    # write to summary database file
    summaryFile = open(summary_json, 'w')
    json.dump(summaryDB, summaryFile, indent=4)
    summaryFile.close()


    # copy html summary page to results directory
    shutil.copy(summaryHtml, resultsDataBasePath)

    # copy js directory to results directory
    jsDir = 'js'
    distutils.dir_util.copy_tree(jsDir, os.path.join(resultsDataBasePath, jsDir))

    # ====== old code ======
    # The following two commands (asciidocDBpy and asciidoctor) should be removed soon:
    # they produce the resultsDB.hmtl file in two steps using the results_json file:
    # (json file ---(asciidocDBpy)--> asciidoc file --(asciidoctor)--> html file)
    # This process is to be replaced with a simpler approach, similar to what happens for
    # summary.html: a generic html page with css and javascript to dynamically populate it using the json file

    # external scripts execution
    cmd = 'python ' + asciidocDBpy + ' -i ' + results_json
    print '----------------------------------------' + asciidocDBpy + '--------------------------------------------'
    testAngioTKPipeline.executeCommand("", cmd, resultsLogPath) # output is a .adoc file named like input

    results_adoc = os.path.splitext(results_json)[0] + '.adoc'
    cmd = 'asciidoctor ' + results_adoc
    print '----------------------------------------' + 'asciidoctor' + '--------------------------------------------'
    testAngioTKPipeline.executeCommand("", cmd, resultsLogPath) # output is a .html file named like input

    # =====================

    # ===== new code ======

    # copy css directory to results directory
    cssDir = 'css'
    distutils.dir_util.copy_tree(cssDir, os.path.join(resultsDataBasePath, cssDir))

    # copy resultsDB2.html file to results directory
    resultsDBHtml = 'resultsDB2.html'
    distutils.file_util.copy_file(resultsDBHtml, os.path.join(resultsDBPath, resultsDBHtml))

    # copy resultsDB3.html file to results directory
    resultsDBHtml = 'resultsDB3.html'
    distutils.file_util.copy_file(resultsDBHtml, os.path.join(resultsDBPath, resultsDBHtml))

    # =====================

    print "Total wall-clock time in " + __file__ + ": " + str(datetime.datetime.now()-initialTime)

# Only do this, if we do not import as a module
if __name__ == "__main__":
    main()
